処理名,処理名,理由,
Heterogeneous Decay(不均一な時定数),大,[v72] 層内のニューロンに「反応が速い/遅い」の個体差を持たせたことで、表現力が飛躍的に向上しました。静止画でも文脈と特徴を同時に捉えられます。,
Rectified Negative Error(誤発火のみ抑制),大,"[v63, v64] 「間違っている上に、興奮している（>0.0）」ニューロンだけを叩き、沈黙しているニューロンは無視する方針が、学習の安定性を劇的に高めました。",
Conservative Pruning(保守的な刈り込み),大,[v74] プルーニング率を 2.5%以下 に抑え、かつ Epoch 2の1回のみ に限定することで、学習済みの回路破壊を防ぎつつ、ノイズ除去に成功しました。,
Wide Margin(マージン最大化),中,[v74] 正解目標(+2.0)と不正解目標(-1.0)の差を広げることで、ノイズに対する頑健性が増しました。,
Single Path(シングルパス),中,[v69] 複雑な「Dual Path / Ensemble」よりも、シンプルな一本道の方が、リソースが集中し、結果的に精度と速度の両面で有利でした。,
,,,
Stochastic Resonance(確率共鳴/ノイズ注入),悪化,[v68] MNISTのような「ノイズの少ない静止画」に対しては、ランダムなノイズ注入は単なる情報の劣化（妨害）にしかなりませんでした。,
Aggressive Pruning(5%以上の刈り込み),悪化,1000サンプルという少ないデータでは、回路の冗長性が低いため、5%削るだけで必要なパスまで切断され、回復不能なダメージを受けました。,
Synaptic Scaling(シナプス正規化),微妙,[v75] 直近のテスト。重みを強制的に整えることで、突出した特徴（尖った強み）が丸められてしまい、キレが悪くなった可能性があります。,
Aggressive LR Decay(急激な学習率減衰),悪化,[v73] 早期に学習率を下げすぎると、局所解（Local Minima）から抜け出せなくなり、90%付近で停滞しました。,