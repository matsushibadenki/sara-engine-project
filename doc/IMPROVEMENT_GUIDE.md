# SARA Engine - 95%精度達成のための改善ガイド

## 現状分析

### 達成済み
- 92.8%の精度（優れた基礎性能）
- GPUなし、誤差逆伝播なしでの学習成功
- 生物学的に妥当なスパイキングニューラルネットワーク

### 課題
- 目標95%まであと2.2%
- エポック2で精度が下がる（90.20%）→ 過学習の兆候
- プルーニングが強すぎる可能性

## 主要な改善点

### 1. アーキテクチャの最適化

#### レイヤー構成の変更
```python
# 旧: 3層、合計5000ニューロン
[1500, 2000, 1500]

# 新: 4層、合計6000ニューロン
[1200, 1800, 1800, 1200]
```

**理由:**
- より多様な時定数での処理が可能
- 異なる時間スケールでのパターン認識
- 総ニューロン数を増やして表現力向上

#### 時定数の調整
```python
# 旧: [0.3, 0.7, 0.95]
# 新: [0.25, 0.5, 0.75, 0.92]
```

**理由:**
- より細かい時間スケールの分離
- 高速層(0.25)で瞬時的特徴を捉える
- 超低速層(0.92)で長期的文脈を保持

### 2. 学習メカニズムの改善

#### 閾値の最適化
```python
# 旧: [0.8, 0.8, 1.0]
# 新: [0.6, 0.7, 0.8]
```

**効果:**
- 発火しやすくなり、より豊かな表現
- 情報伝達の向上

#### 学習率の調整
```python
# 旧: 0.001
# 新: 0.0015 + 減衰スケジューリング
```

**効果:**
- 初期は速く学習
- 徐々に細かい調整へ移行

#### ターゲット値の強化
```python
# 旧: target=1.0, others=-0.1
# 新: target=1.5, others=-0.2
```

**効果:**
- クラス間のマージンを拡大
- より明確な識別境界

### 3. 正則化の改善

#### プルーニング率の調整
```python
# 旧: 5% (固定)
# 新: 4% → 3% (エポックごとに減少)
```

**効果:**
- 初期は積極的に刈り込み
- 後期は重要な結合を保護

#### 重み減衰の緩和
```python
# 旧: 0.995
# 新: 0.997
```

**効果:**
- 学習した情報の保持
- 過度な忘却を防ぐ

### 4. 入力エンコーディングの改善

#### タイムステップの増加
```python
# 旧: 50 steps
# 新: 60 steps
```

**効果:**
- より詳細な時間情報
- パターンの識別精度向上

#### スパイク率の調整
```python
# 旧: rate = img * 0.4
# 新: rate = img * 0.5
```

**効果:**
- より多くのスパイクで情報量増加
- 低輝度ピクセルの表現改善

### 5. 訓練戦略の改善

#### エポック数とサンプル数
```python
# 旧: 3 epochs × 5000 samples = 15,000
# 新: 5 epochs × 8000 samples = 40,000
```

**効果:**
- 十分な学習機会
- より多様なパターンへの露出

#### 適応的Dropout
```python
# エポック1-2: 0.10
# エポック3-5: 0.08
```

**効果:**
- 初期は汎化性能重視
- 後期は精度向上重視

## 予想される性能向上

### 段階的な改善予測

1. **タイムステップ増加 (50→60)**: +0.5%
2. **レイヤー構成最適化**: +0.8%
3. **学習率・ターゲット調整**: +0.6%
4. **訓練データ量増加**: +0.5%
5. **その他の細かい調整**: +0.3%

**合計予想改善: +2.7%**
**予想最終精度: 95.5%前後**

## 実装手順

### ステップ1: core.pyの置き換え
```bash
# バックアップを取る
cp src/sara_engine/core.py src/sara_engine/core.py.backup

# 改善版をコピー
cp core_improved.py src/sara_engine/core.py
```

### ステップ2: train_mnist.pyの更新
```bash
cp train_mnist_improved.py examples/train_mnist.py
```

### ステップ3: 実行
```bash
python examples/train_mnist.py --epochs 5 --samples 8000
```

## さらなる改善のアイデア

### もし95%に届かない場合

1. **エポック数を増やす**: 7-10エポック
2. **データ拡張**: 回転、平行移動の追加
3. **アンサンブル**: 複数モデルの投票
4. **時定数の微調整**: グリッドサーチ
5. **Lateral inhibition**: 競合的学習の強化

### 長期的な改善

1. **STDP (Spike-Timing-Dependent Plasticity)**: より生物学的な学習則
2. **Adaptive neuron models**: より複雑なニューロンモデル
3. **Hierarchical features**: 階層的特徴抽出
4. **Attention mechanisms**: スパイク版アテンション

## トラブルシューティング

### 精度が下がる場合
- 学習率を0.001に下げる
- プルーニング率を0.02に下げる
- エポック数を減らす

### 訓練が遅い場合
- サンプル数を6000に減らす
- タイムステップを50に戻す
- NumPyの最適化を確認

### メモリ不足の場合
- レイヤーサイズを[1000, 1500, 1500, 1000]に減らす
- バッチ処理を実装

## 性能ベンチマーク

### 予想される訓練時間（M4 MacBook Air）

```
5 epochs × 8000 samples × 60 steps
≈ 600-700秒 (10-12分)
処理速度: 45-50 img/s
```

## まとめ

この改善版では、生物学的妥当性を保ちながら:

1. ✅ より豊かな時間的ダイナミクス
2. ✅ 適応的な学習戦略
3. ✅ バランスの取れた正則化
4. ✅ 十分な訓練機会

これらにより **95%精度の達成が十分可能** です。

実験結果を見て、さらなる微調整を行ってください！
