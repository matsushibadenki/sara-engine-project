# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/test_math_chat.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: æ•°å¼å­¦ç¿’ç¢ºèªç”¨ãƒãƒ£ãƒƒãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: å­¦ç¿’ã—ãŸæ•°å¼ã®çŸ¥è­˜ï¼ˆLaTeXã¨è‡ªç„¶è¨€èªã®çµã³ã¤ãï¼‰ãŒæ­£ã—ãå¼•ãå‡ºã›ã‚‹ã‹ã‚’å¯¾è©±å½¢å¼ã§ç¢ºèªã™ã‚‹ã€‚

import torch
import msgpack
import os
from transformers import AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM

def run_math_chat():
    model_path = "models/distilled_sara_llm.msgpack"
    
    if not os.path.exists(model_path):
        print(f"âŒ '{model_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«å­¦ç¿’ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
        return
        
    print("Initializing SNN Model (8192 neurons)...")
    student = SpikingLLM(num_layers=2, sdr_size=8192, vocab_size=256000)
    
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
    
    print(f"Opening SNN memory file: {model_path}...")
    with open(model_path, "rb") as f:
        state = msgpack.unpack(f, raw=False)
    
    raw_map = state.get("direct_map", {})
    fixed_map = {}
    for k, v in raw_map.items():
        fixed_map[eval(k)] = {int(tk): float(tv) for tk, tv in v.items()}
    student._direct_map = fixed_map
    print(f"âœ… Loaded {len(fixed_map)} patterns.")

    print("\n=======================================================")
    print("ğŸ¤– SARA Engine æ•°å¼å­¦ç¿’ãƒ†ã‚¹ãƒˆãƒãƒ£ãƒƒãƒˆã¸ã‚ˆã†ã“ãï¼")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("=======================================================\n")
    
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit']:
            break
            
        prompt = f"You: {user_input}\nSARA:"
        
        # SNNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®æ¨è«–
        input_ids = tokenizer(prompt)["input_ids"]
        context_tokens = input_ids.copy()
        
        print("SARA: ", end="", flush=True)
        
        generated_tokens = []
        for _ in range(100):  # æœ€å¤§100ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
            # ç›´è¿‘ã®8ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
            ctx = context_tokens[-8:] if len(context_tokens) > 8 else context_tokens
            sdr_k = student._sdr_key(student._encode_to_sdr(ctx))
            
            if sdr_k in student._direct_map and student._direct_map[sdr_k]:
                # æœ€ã‚‚é‡ã¿ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ
                next_token = max(student._direct_map[sdr_k].items(), key=lambda x: x[1])[0]
            else:
                # æœªçŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆã¯æ¨è«–çµ‚äº†
                break
                
            generated_tokens.append(next_token)
            context_tokens.append(next_token)
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦é †æ¬¡è¡¨ç¤º
            text_chunk = tokenizer.decode([next_token])
            print(text_chunk, end="", flush=True)
            
            # æ”¹è¡ŒãŒç”Ÿæˆã•ã‚ŒãŸã‚‰å›ç­”ã®åŒºåˆ‡ã‚Šã¨ã¿ãªã™
            if next_token == tokenizer.encode("\n", add_special_tokens=False)[-1] or "\n" in text_chunk:
                break
                
        print()

if __name__ == "__main__":
    run_math_chat()