# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/sara_cli.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: SARAçµ±åˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: ãƒ‡ãƒ¼ã‚¿åé›†ã€çµ±åˆã€å­¦ç¿’ã€æ¨è«–ãƒ†ã‚¹ãƒˆã€ãã—ã¦è¨˜æ†¶ã®åˆˆã‚Šè¾¼ã¿ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ãªã©ã®å‡¦ç†ã‚’ä¸€å…ƒç®¡ç†ã™ã‚‹ã€‚

import argparse
import sys
import os
import shutil

# scriptsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè‡ªä½“ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã«è¿½åŠ ã—ã€ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦èªè­˜ã•ã›ã‚‹
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from data.collect_math import generate_math_corpus, default_math_database
from data.collect_all import CorpusIntegrator
from data.collect_docs import process_document
from train.train_chat import train_chat_data
from train.train_vision import train_vision_association
from eval.test_math_chat import run_math_chat
from eval.test_vision_inference import run_vision_inference
from utils.prune_memory import prune_model_memory  # ğŸ’¡ æ–°è¦è¿½åŠ 

def main():
    parser = argparse.ArgumentParser(description="SARA Engine çµ±åˆç®¡ç†CLI - Professional Edition")
    subparsers = parser.add_subparsers(dest="command", help="å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰")

    # 1. æ•°å¼ã‚³ãƒ¼ãƒ‘ã‚¹ç”Ÿæˆ
    parser_math = subparsers.add_parser("generate-math", help="æ•°å¼ã®ãƒ†ã‚­ã‚¹ãƒˆã¨Q&Aã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚")
    parser_math.add_argument("--out_txt", default="data/interim/math_corpus.txt")
    parser_math.add_argument("--out_jsonl", default="data/interim/math_corpus.jsonl")

    # 2. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæŠ½å‡º
    parser_docs = subparsers.add_parser("extract-docs", help="å¤šæ§˜ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
    parser_docs.add_argument("type", choices=["pdf", "csv", "html"], help="ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼")
    parser_docs.add_argument("source", help="ãƒ‘ã‚¹ã¾ãŸã¯URL")
    parser_docs.add_argument("--out_txt", default="data/interim/docs_corpus.txt")

    # 3. ã‚³ãƒ¼ãƒ‘ã‚¹çµ±åˆ
    parser_integrate = subparsers.add_parser("integrate-corpus", help="interimå†…ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é«˜å“è³ªãªå­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚")
    parser_integrate.add_argument("--out_corpus", default="data/processed/corpus.txt", help="å‡ºåŠ›å…ˆ")
    parser_integrate.add_argument("--dir", default="data/interim", help="ã‚¹ã‚­ãƒ£ãƒ³å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    # 4. å¯¾è©±å­¦ç¿’ (ãƒ†ã‚­ã‚¹ãƒˆ)
    parser_train = subparsers.add_parser("train-chat", help="ãƒãƒ£ãƒƒãƒˆ/æ•°å¼ãƒ‡ãƒ¼ã‚¿ã‚’SNNã«è’¸ç•™å­¦ç¿’ã•ã›ã¾ã™ã€‚")
    parser_train.add_argument("--sources", nargs="+", default=["data/raw/chat_data.jsonl", "data/interim/math_corpus.jsonl"])
    parser_train.add_argument("--model", default="models/distilled_sara_llm.msgpack")

    # 5. å¯¾è©±ãƒ†ã‚¹ãƒˆ (ãƒãƒ£ãƒƒãƒˆUIé¢¨)
    parser_chat = subparsers.add_parser("chat", help="å­¦ç¿’æ¸ˆã¿SNNãƒ¢ãƒ‡ãƒ«ã¨å¯¾è©±ã‚’è¡Œã„ã¾ã™ã€‚")
    parser_chat.add_argument("--model", default="models/distilled_sara_llm.msgpack")

    # 6. è¦–è¦šé€£æƒ³å­¦ç¿’
    parser_vtrain = subparsers.add_parser("train-vision", help="ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã‚’é€£æƒ³è¨˜æ†¶ã¨ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚")
    parser_vtrain.add_argument("--csv", default="data/raw/visual/text/captions.csv")
    parser_vtrain.add_argument("--img_dir", default="data/raw/visual/images")
    parser_vtrain.add_argument("--model", default="models/distilled_sara_llm.msgpack")

    # 7. è¦–è¦šæ¨è«–ãƒ†ã‚¹ãƒˆ
    parser_vtest = subparsers.add_parser("vision-test", help="ç”»åƒã‹ã‚‰SARAã®é€£æƒ³ï¼ˆèªè­˜ï¼‰ã‚’ç¢ºèªã—ã¾ã™ã€‚")
    parser_vtest.add_argument("image", help="ãƒ†ã‚¹ãƒˆç”»åƒãƒ‘ã‚¹")
    parser_vtest.add_argument("--model", default="models/distilled_sara_llm.msgpack")

    # 8. è¨˜æ†¶ã®åˆˆã‚Šè¾¼ã¿ (æ–°è¦è¿½åŠ )
    parser_prune = subparsers.add_parser("prune", help="é‡ã¿ã®ä½ã„ä¸è¦ãªè¨˜æ†¶ã‚’å‰Šé™¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã—ã¾ã™ã€‚")
    parser_prune.add_argument("--model", default="models/distilled_sara_llm.msgpack", help="å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«")
    parser_prune.add_argument("--threshold", type=float, default=50.0, help="å‰Šé™¤ã™ã‚‹é‡ã¿ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50.0ï¼‰")

    # 9. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚³ãƒãƒ³ãƒ‰
    parser_clean = subparsers.add_parser("clean", help="ä¸­é–“ãƒ‡ãƒ¼ã‚¿ã‚„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¦ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚")
    parser_clean.add_argument("--all", action="store_true", help="processedãƒ‡ãƒ¼ã‚¿ã‚‚ã™ã¹ã¦å‰Šé™¤ã—ã¾ã™ã€‚")

    args = parser.parse_args()

    if args.command == "generate-math":
        generate_math_corpus(default_math_database, args.out_txt, args.out_jsonl)
    elif args.command == "extract-docs":
        process_document(args.type, args.source, args.out_txt)
    elif args.command == "integrate-corpus":
        print(f"--- ã‚³ãƒ¼ãƒ‘ã‚¹çµ±åˆã‚’é–‹å§‹ã—ã¾ã™ ({args.dir} -> {args.out_corpus}) ---")
        integrator = CorpusIntegrator(output_path=args.out_corpus)
        if os.path.exists(args.dir):
            files = [f for f in os.listdir(args.dir) if f.endswith(".txt")]
            for filename in sorted(files):
                file_path = os.path.join(args.dir, filename)
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    source_type = "math" if "math" in filename else "document"
                    integrator.add_source(content, source_type=source_type)
        else:
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.dir}")
    elif args.command == "train-chat":
        train_chat_data(args.sources, args.model)
    elif args.command == "chat":
        run_math_chat(args.model)
    elif args.command == "train-vision":
        train_vision_association(args.csv, args.img_dir, args.model)
    elif args.command == "vision-test":
        run_vision_inference(args.image, args.model)
    elif args.command == "prune":
        prune_model_memory(args.model, args.threshold)
    elif args.command == "clean":
        print("--- ç’°å¢ƒã®ãƒªã‚»ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã™ ---")
        targets = ["data/interim"]
        if args.all:
            targets.append("data/processed")
        for target in targets:
            if os.path.exists(target):
                for item in os.listdir(target):
                    if item == ".gitkeep": continue
                    path = os.path.join(target, item)
                    if os.path.isdir(path):
                        shutil.rmtree(path)
                    else:
                        os.remove(path)
                print(f"âœ… {target} ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
    else:
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()