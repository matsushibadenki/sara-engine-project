# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/test_math_chat.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: æ•°å¼å­¦ç¿’ç¢ºèªç”¨ãƒãƒ£ãƒƒãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: SNNãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å­¦ç¿’ã—ãŸçŸ¥è­˜ãŒæ­£ã—ãå¼•ãå‡ºã›ã‚‹ã‹å¯¾è©±å½¢å¼ã§ç¢ºèªã™ã‚‹ã€‚

import torch
import msgpack
import os
from transformers import AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM

def run_math_chat(model_path):
    if not os.path.exists(model_path):
        print(f"âŒ '{model_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
        
    print("Initializing SNN Model (8192 neurons)...")
    student = SpikingLLM(num_layers=2, sdr_size=8192, vocab_size=256000)
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
    
    print(f"Opening SNN memory file: {model_path}...")
    with open(model_path, "rb") as f:
        state = msgpack.unpack(f, raw=False)
    
    raw_map = state.get("direct_map", {})
    student._direct_map = {eval(k): {int(tk): float(tv) for tk, tv in v.items()} for k, v in raw_map.items()}
    print(f"âœ… Loaded {len(student._direct_map)} patterns.")

    print("\n=======================================================")
    print("ðŸ¤– SARA Engine ãƒ†ã‚¹ãƒˆãƒãƒ£ãƒƒãƒˆã¸ã‚ˆã†ã“ãï¼ (çµ‚äº†: quit)")
    print("=======================================================\n")
    
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit']: break
            
        prompt = f"You: {user_input}\nSARA:"
        context_tokens = tokenizer(prompt)["input_ids"].copy()
        print("SARA: ", end="", flush=True)
        
        for _ in range(100):
            ctx = context_tokens[-24:] if len(context_tokens) > 24 else context_tokens
            sdr_k = student._sdr_key(student._encode_to_sdr(ctx))
            
            if sdr_k in student._direct_map and student._direct_map[sdr_k]:
                next_token = max(student._direct_map[sdr_k].items(), key=lambda x: x[1])[0]
            else:
                break
                
            context_tokens.append(next_token)
            text_chunk = tokenizer.decode([next_token])
            print(text_chunk, end="", flush=True)
            
            if next_token == tokenizer.encode("\n", add_special_tokens=False)[-1] or "\n" in text_chunk:
                break
        print()