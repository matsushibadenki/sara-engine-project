# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/collect_all.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: SARAçµ±åˆã‚³ãƒ¼ãƒ‘ã‚¹ãƒ»ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ï¼ˆé«˜å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»ç¦å‰‡å‡¦ç†æ©Ÿèƒ½ä»˜ãï¼‰
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: ç•°ãªã‚‹ã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä¸€æ‰‹ã«å¼•ãå—ã‘ã€ä¸€è²«æ€§ã®ã‚ã‚‹é«˜å“è³ªãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ data/processed/ ã«ä½œæˆã™ã‚‹ã€‚ä¸å®Œå…¨ãªæ–‡ã‚„ãƒã‚¤ã‚ºã‚’å¼·åŠ›ã«å¼¾ãå‡¦ç†ã‚’è¿½åŠ ã€‚

import os
import re

class CorpusIntegrator:
    def __init__(self, output_path):
        self.output_path = output_path
        self.seen_lines = set()
        os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
        
        if os.path.exists(output_path):
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    self.seen_lines.add(line.strip())

    def clean_generic(self, text):
        text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', text)
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def clean_wikipedia(self, text):
        text = re.sub(r'\{\{.*?\}\}', '', text)
        text = re.sub(r'\[\[(?:ãƒ•ã‚¡ã‚¤ãƒ«|ç”»åƒ|File|Image):.*?\]\]', '', text)
        text = re.sub(r'\[\[([^|]*?)\|([^|]*?)\]\]', r'\2', text)
        text = re.sub(r'\[\[(.*?)\]\]', r'\1', text)
        return text

    def clean_arxiv(self, text):
        text = re.sub(r'\$.*?\$', '', text)
        text = re.sub(r'\\[a-zA-Z]+', '', text)
        text = re.sub(r'\{.*?\}', '', text)
        return text
        
    def clean_math(self, text):
        text = re.sub(r'[ \t]+', ' ', text)
        return text.strip()

    def clean_document(self, text):
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[â–¼â–¶â– â—†â—]', '', text)
        return text.strip()

    def add_source(self, raw_text, source_type="generic"):
        if source_type == "math":
            text = self.clean_math(raw_text)
        elif source_type == "document":
            text = self.clean_document(raw_text)
        else:
            text = self.clean_generic(raw_text)
            if source_type == "wikipedia":
                text = self.clean_wikipedia(text)
            elif source_type == "arxiv":
                text = self.clean_arxiv(text)
        
        # ç¦å‰‡å‡¦ç†ï¼šé–‰ã˜ã‚«ãƒƒã‚³é¡ã‚’å‰ã®æ–‡ã®ã‚»ãƒƒãƒˆã«ã™ã‚‹
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ]+)([ã€ï¼‰ã€ã€‘ã€•ã€‰ã€‹\]\)]+)', r'\1\2\n', text)
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ]+)([^ã€ï¼‰ã€ã€‘ã€•ã€‰ã€‹\]\)\n])', r'\1\n\2', text)
        
        new_lines_count = 0
        with open(self.output_path, "a", encoding="utf-8") as f:
            for line in text.split('\n'):
                line = line.strip()
                
                # ğŸ’¡ é«˜å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ 1: æ–‡é ­ã®ã‚´ãƒŸï¼ˆå¥èª­ç‚¹ã€é–‰ã˜ã‚«ãƒƒã‚³ãªã©ï¼‰ã‚’å‰Šé™¤
                line = re.sub(r'^[ã€‚ã€ï¼ï¼Ÿ\sã€ï¼‰ã€ã€‘ã€•ã€‰ã€‹\]\)]+', '', line)
                
                # ğŸ’¡ é«˜å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ 2: æœ‰æ„ç¾©ãªé•·ã•ã®ç¢ºä¿ï¼ˆãƒã‚¤ã‚ºå¼¾ãã®ãŸã‚ã«10æ–‡å­—ä»¥ä¸Šã«è¨­å®šï¼‰
                if len(line) < 10:
                    continue
                    
                # ğŸ’¡ é«˜å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ 3: ä¸è‡ªç„¶ãªæ–‡é ­ã®é™¤å¤–ï¼ˆã€Œã‚’ã€ã€Œã«ã€ã€ŒãŒã€ç­‰ã®åŠ©è©ã‹ã‚‰å§‹ã¾ã‚‹æ–‡ã¯ã€å‰å¾Œã®æ–‡è„ˆãŒåˆ‡æ–­ã•ã‚ŒãŸãƒã‚¤ã‚ºã¨ã¿ãªã™ï¼‰
                if re.match(r'^[ã‚’ã«ãŒã¯ã§ã¸ã¨ã®ã‚‚]', line):
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ãŸã‚‚ã®ã ã‘ã‚’æ›¸ãè¾¼ã‚€
                if line not in self.seen_lines:
                    f.write(line + "\n")
                    self.seen_lines.add(line)
                    new_lines_count += 1
        
        if new_lines_count > 0:
            print(f"ğŸ“¥ {source_type} ã‹ã‚‰ {new_lines_count} æ–‡ã®é«˜å“è³ªãªçŸ¥è­˜ã‚’çµ±åˆã—ã¾ã—ãŸã€‚")
        else:
            print(f"â„¹ï¸ {source_type} ã‹ã‚‰ã®å…¥åŠ›ã¯ã™ã¹ã¦é‡è¤‡ã¾ãŸã¯å“è³ªåŸºæº–ã‚’æº€ãŸã•ãªã‹ã£ãŸãŸã‚è¿½åŠ ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")