# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/train/train_vision.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: è¦–è¦šãƒ»è¨€èªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«é€£æƒ³å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: ImageSpikeEncoderã‚’ä½¿ç”¨ã—ã¦ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£æƒ³å­¦ç¿’ã•ã›ã‚‹ã€‚ä¿å­˜æ™‚ã®å¤‰æ•°åã‚¨ãƒ©ãƒ¼(NameError)ã‚’ä¿®æ­£ã€‚

import os
import csv
import torch
import msgpack
import tqdm
import numpy as np
from PIL import Image
from sara_engine.models.spiking_llm import SpikingLLM
from sara_engine.encoders.vision import ImageSpikeEncoder
from transformers import AutoTokenizer

def train_vision_association(csv_path, image_dir, model_path):
    print("Initializing Multi-modal Training Environment...")
    
    # SNNãƒ¢ãƒ‡ãƒ«ã¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ (8192ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³)
    student = SpikingLLM(num_layers=2, sdr_size=8192, vocab_size=256000)
    vision_encoder = ImageSpikeEncoder(output_size=8192)
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-2b")
    
    # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    if os.path.exists(model_path):
        print(f"Loading existing memory: {model_path}")
        with open(model_path, "rb") as f:
            state = msgpack.unpack(f, raw=False)
        raw_map = state.get("direct_map", {})
        # ä¿å­˜ç”¨ã«æ–‡å­—åˆ—åŒ–ã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ã‚’ã‚¿ãƒ—ãƒ«(intå‹)ã«å¾©å…ƒ
        student._direct_map = {eval(k): {int(tk): float(tv) for tk, tv in v.items()} for k, v in raw_map.items()}
    else:
        student._direct_map = {}

    if not os.path.exists(csv_path):
        print(f"âŒ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return

    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        data_pairs = list(reader)

    print(f"ğŸš€ {len(data_pairs)} ä»¶ã®ãƒšã‚¢ã‚’å­¦ç¿’ã—ã¾ã™...")

    for item in tqdm.tqdm(data_pairs, desc="Vision-Text Pairing"):
        img_name = item['image_file']
        caption = item['caption']
        img_path = os.path.join(image_dir, img_name)
        
        if not os.path.exists(img_path):
            continue

        try:
            # 1. ç”»åƒã‚’èª­ã¿è¾¼ã¿ã€ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®æ•°å€¤ãƒªã‚¹ãƒˆ(0.0-1.0)ã«å¤‰æ›
            image = Image.open(img_path).convert('L').resize((64, 64))
            pixel_values = list(np.array(image).flatten() / 255.0)
            
            # 2. ç‰¹å¾´é‡ã‹ã‚‰SDRã‚’ç”Ÿæˆ
            vision_sdr = vision_encoder.encode(pixel_values)
            vision_key = student._sdr_key(vision_sdr)

            # 3. ãƒ†ã‚­ã‚¹ãƒˆã¨ç´ä»˜ã‘ (åŒæ™‚ç™ºç«ã®åŸç†)
            tokens = tokenizer.encode(caption, add_special_tokens=False)
            if vision_key not in student._direct_map:
                student._direct_map[vision_key] = {}
            
            target_map = student._direct_map[vision_key]
            for token_id in tokens:
                # ç”»åƒSDRã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®çµåˆè·é‡ã‚’å¼·åŒ–
                target_map[token_id] = min(target_map.get(token_id, 0.0) + 800.0, 2000.0)
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ ({img_name}): {e}")

    print("Saving updated memory...")
    # ğŸ’¡ ä¿®æ­£ç‚¹: å†…åŒ…è¡¨è¨˜å†…ã®å¤‰æ•°åã‚’ tv ã‹ã‚‰ v ã«ã€ã¾ãŸã¯ tk, tv ã‹ã‚‰ v.items() ã«ä¿®æ­£
    state = {
        "direct_map": {str(k): {str(tk): v for tk, v in tv.items()} for k, tv in student._direct_map.items()},
        "vocab_size": student.vocab_size
    }
    with open(model_path, "wb") as f:
        msgpack.pack(state, f)
    print("âœ¨ è¦–è¦šé€£æƒ³å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    train_vision_association(
        "data/raw/visual/text/captions.csv",
        "data/raw/visual/images",
        "models/distilled_sara_llm.msgpack"
    )