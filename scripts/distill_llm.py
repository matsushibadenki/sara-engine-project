{
    "//": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/distill_llm.py",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: LLMè’¸ç•™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (å¤§å®¹é‡ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ»ä¸­æ–­å†é–‹å¯¾å¿œç‰ˆ)",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: æ—¢å­˜ã®MessagePackãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ–°ã—ã„ã‚³ãƒ¼ãƒ‘ã‚¹ã®çŸ¥è­˜ã‚’è¿½åŠ ã™ã‚‹ã€‚ã•ã‚‰ã«ã€é•·æ™‚é–“ã®å­¦ç¿’ã‚’è€ƒæ…®ã—ã¦å®šæœŸçš„ãªä¿å­˜ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰ã¨ã€ä¸­æ–­ã—ãŸç®‡æ‰€ã‹ã‚‰ã®å†é–‹æ©Ÿèƒ½ï¼ˆCtrl+Cã‚­ãƒ£ãƒƒãƒï¼‰ã‚’å®Ÿè£…ã€‚"
}

import torch
import msgpack
import os
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM
import tqdm

class SNNLLMDistiller:
    def __init__(
        self, 
        teacher_model_name: str, 
        student_model: SpikingLLM,
        device: str = "cpu"
    ):
        print(f"Loading teacher model: {teacher_model_name} on {device}")
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)
        self.teacher = AutoModelForCausalLM.from_pretrained(
            teacher_model_name, 
            torch_dtype=torch.float32,
            device_map=device
        )
        self.teacher.eval()
        self.student = student_model
        self.device = device

    def load_student(self, path: str):
        if os.path.exists(path):
            print(f"Loading existing SNN memory from {path}...")
            with open(path, "rb") as f:
                state = msgpack.unpack(f, raw=False)
            
            fixed_map = {}
            for str_sdr_k, next_tokens in state["direct_map"].items():
                sdr_k = eval(str_sdr_k)
                fixed_map[sdr_k] = {int(k): float(v) for k, v in next_tokens.items()}
            
            self.student._direct_map = fixed_map
            print(f"Successfully loaded {len(self.student._direct_map)} existing context patterns.")
        else:
            print(f"No existing memory found at {path}. Starting fresh.")

    def distill_single_text(self, text: str, max_length: int = 128, top_k: int = 5):
        # 1æ–‡å˜ä½ã®è’¸ç•™å‡¦ç†ã«åˆ†é›¢
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length).to(self.device)
        input_ids = inputs["input_ids"][0].tolist()

        if len(input_ids) < 2:
            return

        with torch.no_grad():
            outputs = self.teacher(**inputs)
            logits = outputs.logits[0]
            probs = torch.softmax(logits, dim=-1)

        context_tokens = []
        context_window = 8

        for i in range(len(input_ids) - 1):
            current_tok = input_ids[i]
            actual_next_token = input_ids[i + 1]

            context_tokens.append(current_tok)
            if len(context_tokens) > context_window:
                context_tokens.pop(0)

            top_probs, top_indices = torch.topk(probs[i], top_k)
            
            sdr = self.student._encode_to_sdr(context_tokens)
            sdr_k = self.student._sdr_key(sdr)

            if sdr_k not in self.student._direct_map:
                self.student._direct_map[sdr_k] = {}
            
            dm = self.student._direct_map[sdr_k]

            dm[actual_next_token] = dm.get(actual_next_token, 0.0) + 100.0

            for rank in range(top_k):
                target_token = top_indices[rank].item()
                target_prob = top_probs[rank].item()
                
                if target_token != actual_next_token:
                    increment = 10.0 * target_prob
                    dm[target_token] = dm.get(target_token, 0.0) + increment

            for tok_id in list(dm.keys()):
                if tok_id != actual_next_token:
                    dm[tok_id] *= 0.8
                if dm[tok_id] > 200.0:
                    dm[tok_id] = 200.0

    def save_student(self, path: str):
        state = {
            "direct_map": {str(k): {str(tk): v for tk, v in tv.items()} for k, tv in self.student._direct_map.items()},
            "vocab_size": self.student.vocab_size
        }
        with open(path, "wb") as f:
            msgpack.pack(state, f)

def load_corpus(filepath: str) -> list[str]:
    texts = []
    if os.path.exists(filepath):
        print(f"Loading corpus from {filepath}...")
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line:
                    texts.append(line)
        print(f"Loaded {len(texts)} lines.")
    else:
        print(f"Warning: {filepath} not found.")
    return texts

def load_progress(progress_file: str) -> int:
    """é€²æ—çŠ¶æ³ã‚’ä¿å­˜ã—ãŸJSONã‹ã‚‰ã€æœ€å¾Œã«å‡¦ç†ã—ãŸè¡Œç•ªå·ã‚’å–å¾—ã™ã‚‹"""
    if os.path.exists(progress_file):
        try:
            with open(progress_file, "r") as f:
                data = json.load(f)
                return data.get("last_processed_index", 0)
        except Exception:
            return 0
    return 0

def save_progress(progress_file: str, index: int):
    """é€²æ—çŠ¶æ³ã‚’JSONã«ä¿å­˜ã™ã‚‹"""
    with open(progress_file, "w") as f:
        json.dump({"last_processed_index": index}, f)

if __name__ == "__main__":
    student = SpikingLLM(num_layers=2, sdr_size=8192, vocab_size=256000) 
    device = "mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu")
    
    distiller = SNNLLMDistiller(
        teacher_model_name="google/gemma-2-2b",
        student_model=student,
        device=device
    )

    model_path = "distilled_sara_llm.msgpack"
    progress_file = "data/progress.json"
    corpus_file = "data/corpus.txt"

    distiller.load_student(model_path)
    dataset = load_corpus(corpus_file)
    
    if dataset:
        start_index = load_progress(progress_file)
        total_lines = len(dataset)
        
        if start_index >= total_lines:
            print("âœ… ã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã¯ã™ã¹ã¦å­¦ç¿’æ¸ˆã¿ã§ã™ã€‚æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹ã‹ã€progress.json ã‚’å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚")
        else:
            print(f"ğŸš€ å­¦ç¿’ã‚’å†é–‹ã—ã¾ã™: {start_index + 1} è¡Œç›®ã‹ã‚‰ {total_lines} è¡Œç›®ã¾ã§")
            
            # å®šæœŸä¿å­˜ã®é–“éš”ï¼ˆä½•è¡Œã”ã¨ã«ã‚»ãƒ¼ãƒ–ã™ã‚‹ã‹ï¼‰
            save_interval = 100
            
            try:
                for i in tqdm.tqdm(range(start_index, total_lines), desc="Distilling", initial=start_index, total=total_lines):
                    distiller.distill_single_text(dataset[i])
                    
                    # å®šæœŸä¿å­˜å‡¦ç† (100è¡Œã”ã¨)
                    if (i + 1) % save_interval == 0:
                        distiller.save_student(model_path)
                        save_progress(progress_file, i + 1)
                
                # ãƒ«ãƒ¼ãƒ—ãŒæœ€å¾Œã¾ã§çµ‚ã‚ã£ãŸæ™‚ã®æœ€çµ‚ä¿å­˜
                distiller.save_student(model_path)
                save_progress(progress_file, total_lines)
                print(f"âœ… ã™ã¹ã¦ã®å­¦ç¿’ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
                
            except KeyboardInterrupt:
                # Ctrl+C ã§å®‰å…¨ã«ä¸­æ–­ã™ã‚‹å‡¦ç†
                print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦å­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
                print("ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™...")
                distiller.save_student(model_path)
                save_progress(progress_file, i)
                print(f"âœ… {i}è¡Œç›®ã¾ã§ã®é€²æ—ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚æ¬¡å›ã¯ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
    else:
        print("âŒ ã‚³ãƒ¼ãƒ‘ã‚¹ãŒç©ºã®ãŸã‚ã€è’¸ç•™ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")