# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/distill_llm.py
# ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: LLMè’¸ç•™ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (SQLite DBå¯¾å¿œãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¿®æ­£ç‰ˆ)
# ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã€‚SQLiteã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’é€æ¬¡èª­ã¿è¾¼ã¿ã€SNNã¸è’¸ç•™ã™ã‚‹ã€‚

import torch
import msgpack
import os
import json
import tqdm
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM

# ğŸ’¡ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®å‡¦ç†
# å®Ÿè¡Œä¸­ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# ã“ã‚Œã§ ModuleNotFoundError: No module named 'scripts' ãŒå‡ºãªããªã‚Šã¾ã™
from manage_db import SaraCorpusDB

class SNNLLMDistiller:
    def __init__(self, teacher_model_name, student_model, device="cpu"):
        print(f"Loading teacher model: {teacher_model_name} on {device}")
        self.tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)
        self.teacher = AutoModelForCausalLM.from_pretrained(
            teacher_model_name, 
            torch_dtype=torch.float32, 
            device_map=device
        )
        self.teacher.eval()
        self.student = student_model
        self.device = device

    def load_student(self, path):
        """æ—¢å­˜ã®MessagePackã‹ã‚‰è¨˜æ†¶ã‚’å¾©å…ƒã™ã‚‹"""
        if os.path.exists(path):
            print(f"Opening SNN memory file: {path}...")
            with open(path, "rb") as f:
                state = msgpack.unpack(f, raw=False)
            
            raw_map = state.get("direct_map", {})
            print(f"Restoring {len(raw_map)} context patterns...")
            
            fixed_map = {}
            for k, v in tqdm.tqdm(raw_map.items(), desc="Loading SNN Memory"):
                fixed_map[eval(k)] = {int(tk): float(tv) for tk, tv in v.items()}
            
            self.student._direct_map = fixed_map
            print(f"âœ… Successfully loaded memory.")
            del state
        else:
            print(f"No existing memory found at {path}. Starting fresh.")

    def save_student(self, path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’MessagePackå½¢å¼ã§ä¿å­˜"""
        print(f"Saving SNN memory to {path}...")
        os.makedirs(os.path.dirname(path), exist_ok=True)
        state = {
            "direct_map": {str(k): {str(tk): v for tk, v in tv.items()} for k, tv in self.student._direct_map.items()},
            "vocab_size": self.student.vocab_size
        }
        with open(path, "wb") as f:
            msgpack.pack(state, f)
        print("âœ… Save completed.")

    def distill_single_text(self, text):
        """1æ–‡ã®è’¸ç•™å‡¦ç†"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=128).to(self.device)
        input_ids = inputs["input_ids"][0].tolist()
        if len(input_ids) < 2: return

        with torch.no_grad():
            outputs = self.teacher(**inputs)
            logits = outputs.logits[0]
            probs = torch.softmax(logits, dim=-1)

        context_tokens = []
        for i in range(len(input_ids) - 1):
            context_tokens.append(input_ids[i])
            if len(context_tokens) > 8: context_tokens.pop(0)
            
            sdr_k = self.student._sdr_key(self.student._encode_to_sdr(context_tokens))
            if sdr_k not in self.student._direct_map:
                self.student._direct_map[sdr_k] = {}
            
            dm = self.student._direct_map[sdr_k]
            actual = input_ids[i+1]
            
            # æ­£è§£ãƒ©ãƒ™ãƒ«ã®é‡ã¿ä»˜ã‘
            dm[actual] = min(dm.get(actual, 0.0) + 100.0, 200.0)
            
            # ã‚½ãƒ•ãƒˆãƒ©ãƒ™ãƒ«ï¼ˆå‘¨å›²ã®ç¢ºç‡ï¼‰ã®é‡ã¿ä»˜ã‘
            top_probs, top_indices = torch.topk(probs[i], 5)
            for rank in range(5):
                t_idx = top_indices[rank].item()
                if t_idx != actual:
                    dm[t_idx] = min(dm.get(t_idx, 0.0) + 10.0 * top_probs[rank].item(), 200.0)

if __name__ == "__main__":
    # ãƒ‘ã‚¹è¨­å®š
    model_path = "models/distilled_sara_llm.msgpack"
    data_dir = "data"
    progress_file = os.path.join(data_dir, "progress.json")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    student = SpikingLLM(num_layers=2, sdr_size=8192, vocab_size=256000)
    device = "mps" if torch.backends.mps.is_available() else ("cuda" if torch.cuda.is_available() else "cpu")
    
    distiller = SNNLLMDistiller("google/gemma-2-2b", student, device)
    distiller.load_student(model_path)

    # DBæ¥ç¶š
    db = SaraCorpusDB()
    last_id = 0
    if os.path.exists(progress_file):
        with open(progress_file, "r") as f:
            last_id = json.load(f).get("last_id", 0)

    print(f"ğŸš€ Distilling from DB (Starting ID: {last_id})")
    
    try:
        # DBã‹ã‚‰æœªå­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        cur = db.conn.execute("SELECT id, content FROM corpus WHERE id > ? ORDER BY id", (last_id,))
        rows = cur.fetchall()
        
        if not rows:
            print("âœ… No new data to distill.")
        else:
            for i, row in enumerate(tqdm.tqdm(rows, desc="Overall Progress")):
                distiller.distill_single_text(row[1])
                
                # 50ä»¶ã”ã¨ã«ä¿å­˜
                if (i + 1) % 50 == 0:
                    distiller.save_student(model_path)
                    with open(progress_file, "w") as f:
                        json.dump({"last_id": row[0]}, f)
            
            # æœ€å¾Œã«ä¿å­˜
            distiller.save_student(model_path)
            with open(progress_file, "w") as f:
                json.dump({"last_id": rows[-1][0]}, f)
            print("âœ¨ Distillation completed successfully.")

    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted. Saving current progress...")
        distiller.save_student(model_path)
        # æœ€å¾Œã«å‡¦ç†ã—ãŸè¡Œã®IDã‚’è¨˜éŒ²
        # ã“ã“ã§ã¯ i ãŒãƒ«ãƒ¼ãƒ—å†…å¤‰æ•°ãªã®ã§ã€ç›´å‰ã® row[0] ã‚’ä½¿ã†ãªã©ã®å·¥å¤«ãŒå¿…è¦ã§ã™ãŒ
        # ç°¡æ˜“çš„ã«ä¸­æ–­æ™‚ã®ä¿å­˜ã‚’è¡Œã„ã¾ã™