_FILE_INFO = {
    "//": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/chat_distilled_model.py",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: è’¸ç•™æ¸ˆã¿SNNãƒ¢ãƒ‡ãƒ«ã¨ã®å¯¾è©±ã‚¹ã‚¯ãƒªãƒ—ãƒˆ",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã€SNNãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç¶šãã‚’ç”Ÿæˆã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚"
}

import json
from transformers import AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM

def run_chat():
    print("Loading tokenizer and model...")
    model_name = "google/gemma-2-2b"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è’¸ç•™æ™‚ã¨åŒã˜è¨­å®šã§åˆæœŸåŒ–
    student = SpikingLLM(num_layers=2, sdr_size=256, vocab_size=256000)
    
    print("Loading distilled knowledge...")
    try:
        with open("distilled_sara_llm.json", "r", encoding="utf-8") as f:
            state = json.load(f)
            
        fixed_direct_map = {}
        for str_sdr_k, next_tokens in state["direct_map"].items():
            sdr_k = eval(str_sdr_k) 
            fixed_next_tokens = {int(tok_id): float(count) for tok_id, count in next_tokens.items()}
            fixed_direct_map[sdr_k] = fixed_next_tokens
        
        student._direct_map = fixed_direct_map
        print(f"âœ… Successfully loaded {len(student._direct_map)} context patterns.")
    except FileNotFoundError:
        print("âŒ Error: 'distilled_sara_llm.json' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« distill_llm.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    print("\n" + "="*50)
    print("ğŸ§  SARA SNN Chat Session Started")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    print("="*50 + "\n")

    while True:
        try:
            user_input = input("You: ")
        except (KeyboardInterrupt, EOFError):
            print("\nExiting...")
            break

        if user_input.strip().lower() in ["quit", "exit"]:
            print("ãƒãƒ£ãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")
            break
            
        if not user_input.strip():
            continue

        # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        inputs = tokenizer(user_input, return_tensors="pt")
        prompt_tokens = inputs["input_ids"][0].tolist()

        print(f"SARA: ", end="", flush=True)
        
        current_tokens = prompt_tokens.copy()
        
        # 1æ–‡å­—ãšã¤ç”Ÿæˆã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã™ã‚‹
        for step in range(20):  # å®‰å…¨ã®ãŸã‚æœ€å¤§20ãƒˆãƒ¼ã‚¯ãƒ³ã§åŒºåˆ‡ã‚‹
            # SNNã«ã‚ˆã‚‹1ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬
            next_id = student.generate(
                prompt_tokens=current_tokens, 
                max_new_tokens=1,
                temperature=0.01,
                top_k=1
            )[0]
            
            current_tokens.append(next_id)
            generated_word = tokenizer.decode([next_id])
            
            # ç”»é¢ã«å‡ºåŠ›
            print(generated_word, end="", flush=True)
            
            # çµ‚äº†æ¡ä»¶ã®åˆ¤å®š
            if generated_word.strip() in ["ã€‚", "ï¼", "ï¼Ÿ", "!", "?", "\n"]:
                break
                
        print() # 1å›ã®å¿œç­”ãŒçµ‚ã‚ã£ãŸã‚‰æ”¹è¡Œ

if __name__ == "__main__":
    run_chat()