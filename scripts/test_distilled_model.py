_FILE_INFO = {
    "//": "ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹: scripts/test_distilled_model.py",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: è’¸ç•™æ¸ˆã¿SNNãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆçµ‚äº†æ¡ä»¶è¿½åŠ ç‰ˆï¼‰",
    "//": "ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›®çš„ã‚„å†…å®¹: JSONã‹ã‚‰è’¸ç•™æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å¾©å…ƒã—ã€Gemmaã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ç”¨ã„ã¦1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ç”Ÿæˆã‚’è¡Œã†ã€‚æ–‡ã®çµ‚ã‚ã‚Šï¼ˆå¥ç‚¹ï¼‰ã‚’æ¤œå‡ºã—ãŸã‚‰æ­£å¸¸ã«ç”Ÿæˆã‚’çµ‚äº†ã™ã‚‹ã€‚"
}

import json
import torch
from transformers import AutoTokenizer
from sara_engine.models.spiking_llm import SpikingLLM

def test_inference():
    # 1. è’¸ç•™æ™‚ã¨åŒã˜è¨­å®šã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™
    model_name = "google/gemma-2-2b"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # è’¸ç•™æ™‚ã¨åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆsdr_sizeç­‰ï¼‰ã§åˆæœŸåŒ–
    student = SpikingLLM(num_layers=2, sdr_size=256, vocab_size=256000)
    
    # 2. è’¸ç•™æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‹ã®ä¿®æ­£
    print("Loading distilled model...")
    with open("distilled_sara_llm.json", "r", encoding="utf-8") as f:
        state = json.load(f)
    
    # JSONã§æ–‡å­—åˆ—åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ã¨å€¤ã‚’æ•°å€¤ã«æˆ»ã™
    fixed_direct_map = {}
    for str_sdr_k, next_tokens in state["direct_map"].items():
        # SDRã‚­ãƒ¼ï¼ˆã‚¿ãƒ—ãƒ«ï¼‰ã®å¾©å…ƒ
        sdr_k = eval(str_sdr_k) 
        
        # æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³IDï¼ˆintï¼‰ã¨ã‚«ã‚¦ãƒ³ãƒˆï¼ˆfloatï¼‰ã®å¾©å…ƒ
        fixed_next_tokens = {int(tok_id): float(count) for tok_id, count in next_tokens.items()}
        fixed_direct_map[sdr_k] = fixed_next_tokens
    
    student._direct_map = fixed_direct_map
    print(f"Successfully loaded {len(student._direct_map)} context patterns.")

    # 3. ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
    train_text = "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã¨ã¦ã‚‚è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚æ•£æ­©ã«è¡Œãã®ãŒæ¥½ã—ã¿ã§ã™ã€‚"
    inputs = tokenizer(train_text, return_tensors="pt")
    full_tokens = inputs["input_ids"][0].tolist()
    
    # æœ€åˆã®6ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ä½¿ç”¨
    prompt_length = 6
    prompt_tokens = full_tokens[:prompt_length]
    prompt_text = tokenizer.decode(prompt_tokens)
    
    print(f"\nPrompt tokens: {prompt_tokens}")
    print(f"Prompt text: {prompt_text}")

    # 4. ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®ç”Ÿæˆ
    print("\n--- Generation Step-by-Step ---")
    current_tokens = prompt_tokens.copy()
    context_window = 8
    
    for step in range(15):  # æœ€å¤§15ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
        # ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç›´è¿‘8ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’å–å¾—
        context_tokens = current_tokens[-context_window:]
        sdr = student._encode_to_sdr(context_tokens)
        sdr_k = student._sdr_key(sdr)
        
        # HIT/MISS ã®åˆ¤å®š
        if sdr_k in student._direct_map:
            print(f"Step {step+1}: âœ… HIT  | Context: {tokenizer.decode(context_tokens)}")
        else:
            print(f"Step {step+1}: âŒ MISS | Context: {tokenizer.decode(context_tokens)}")
            
        # 1ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ç”Ÿæˆ (temperatureã‚’æ¥µé™ã¾ã§ä¸‹ã’ã¦æ±ºå®šè«–çš„ã«)
        next_id = student.generate(
            prompt_tokens=current_tokens, 
            max_new_tokens=1,
            temperature=0.01,
            top_k=1
        )[0]
        
        current_tokens.append(next_id)
        generated_word = tokenizer.decode([next_id])
        print(f"  -> Generated: {generated_word} (ID: {next_id})")
        
        # --- çµ‚äº†æ¡ä»¶ã®åˆ¤å®š ---
        # å¥ç‚¹ã€Œã€‚ã€ã‚„ç‰¹æ®Šãªçµ‚äº†è¨˜å·ãŒå‡ºãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
        if generated_word.strip() == "ã€‚":
            print("\nğŸ‰ æ–‡ã®çµ‚ç«¯ï¼ˆã€‚ï¼‰ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€ç”Ÿæˆã‚’æ­£å¸¸çµ‚äº†ã—ã¾ã™ã€‚")
            break

    print(f"\nFinal text: {tokenizer.decode(current_tokens)}")

if __name__ == "__main__":
    test_inference()